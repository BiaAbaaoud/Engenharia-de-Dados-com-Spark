# ‚ö° Pipeline de Big Data: WeFitness Analytics

Este projeto simula um ecossistema de dados para uma rede fict√≠cia de academias (WeFitness). O objetivo √© processar grandes volumes de dados de check-ins para extrair intelig√™ncia financeira, categorizar clientes e preparar os dados para dashboards executivos.

## üöÄ Objetivo
Transformar dados brutos (Raw Data) em insights acion√°veis, utilizando processamento distribu√≠do com **Apache Spark** e seguindo as melhores pr√°ticas de Engenharia de Dados com a **Arquitetura Medalh√£o**.

---

## üèóÔ∏è Estrutura do Pipeline

O pipeline √© dividido em 3 etapas principais:

1.  **Ingest√£o (Bronze):** Gera√ß√£o de 100.000 registros sint√©ticos e armazenamento em formato **Parquet**.
2.  **Transforma√ß√£o (Silver):** Limpeza, filtragem de valores e cria√ß√£o de categorias (Standard e Premium) atrav√©s de Feature Engineering.
3.  **Agrega√ß√£o (Gold):** Consolida√ß√£o de m√©tricas (Ticket M√©dio e Faturamento Total) e carga final no banco de dados **SQLite**.



---

## üìÇ Descri√ß√£o dos Arquivos

| Arquivo | Fun√ß√£o |
| :--- | :--- |
| `gerar_big_data_wefitness.py` | Gera os dados brutos e cria a Camada Bronze. |
| `transformar_silver.py` | Refina os dados e aplica regras de neg√≥cio (Camada Silver). |
| `gerar_gold_final.py` | Agrega os dados e faz o "Load" no Banco SQL (Camada Gold). |
| `ler_dados_spark.py` | Script utilit√°rio para validar a leitura dos arquivos Parquet. |
| `wefitness_analytics.db` | Banco de Dados SQLite final com os resultados prontos para BI. |

---

## üõ†Ô∏è Ferramentas Utilizadas

* **Python 3.13**
* **Apache Spark (PySpark)**
* **Hadoop (Winutils)** para execu√ß√£o em ambiente Windows.
* **Pandas** para ponte de dados SQL.
* **SQLite** como Data Warehouse simplificado.

---

## üèÉ Como rodar o projeto

1.  Certifique-se de ter o **Spark** instalado e configurado.
2.  Clone o reposit√≥rio: `git clone https://github.com/seu-usuario/Engenharia-de-Dados-com-Spark.git`
3.  Execute os scripts na ordem:
    ```bash
    python gerar_big_data_wefitness.py
    python transformar_silver.py
    python gerar_gold_final.py
    ```

---

## ‚ùì FAQ - Perguntas Frequentes

**1. Por que usar Parquet e n√£o CSV?** O Parquet √© um formato colunar que reduz o espa√ßo em disco e acelera a leitura do Spark em at√© 10x comparado ao CSV.

**2. O que √© a Arquitetura Medalh√£o?** √â uma estrutura de organiza√ß√£o de dados (Bronze/Silver/Gold) que garante qualidade e linhagem dos dados durante o processo.

**3. O Spark roda bem no Windows?** Sim, desde que configurado com os bin√°rios do Hadoop (`winutils.exe` e `hadoop.dll`) nas vari√°veis de ambiente.

**4. O projeto escala para milh√µes de linhas?** Sim! O c√≥digo foi escrito usando a API de DataFrames do Spark, que distribui o processamento independente do volume.

**5. Por que usar SQLite na camada Gold?** Para simular a entrega final em um ambiente SQL relacional, facilitando a conex√£o com ferramentas de BI como Power BI ou Tableau.

**6. Como as categorias VIP foram definidas?** Check-ins acima de R$ 80,00 foram classificados como Premium, e os demais acima de R$ 50,00 como Standard.

---

## üë©‚Äçüíª Desenvolvedora
**Bia Abaaoud** 