from pyspark.sql import SparkSession
import os

# 1. Configurando as variÃ¡veis de ambiente via cÃ³digo para facilitar
pasta_projeto = os.getcwd()
os.environ["HADOOP_HOME"] = os.path.join(pasta_projeto, "Hadoop")
os.environ["path"] += os.path.join(pasta_projeto, "Hadoop", "bin")

# 2. Iniciando a SessÃ£o Spark
spark = SparkSession.builder \
    .appName("PrimeiroContatoBigData") \
    .master("local[*]") \
    .getOrCreate()

print("\nðŸš€ Spark iniciado com sucesso! Alhamdulillah.")

# 3. Criando dados de teste (Simulando 1 milhÃ£o de check-ins)
data = [{"id": i, "unidade": "Academia_" + str(i % 10), "valor": 29.90} for i in range(1000000)]
df = spark.createDataFrame(data)

# 4. Mostrando o poder do processamento
print(f"ðŸ“Š Total de registros processados: {df.count()}")
df.show(5)

# 5. Encerrando a sessÃ£o
spark.stop()